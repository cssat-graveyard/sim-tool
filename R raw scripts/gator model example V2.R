# Author: Brian Waismeyer
# Contact: bwaismeyer@gmail.com

# Date created: 3/20/2015
# Date updated: 3/20/2015

###############################################################################
## SCRIPT OVERVIEW

# goal: Prototype that fits a multinomial logit model to given data and then
#       does a set of simulations to generate predictions for the the outcomes
#       across the range of a given predictor.
#
#       The model fitting and prediction generation is handled by the simcf 
#       package and the example code described in the help docs for the 
#       mlogitsimev function from that package. 
#
#       Where to find the original example code:
#       library(simcf)
#       help(mlogitsimev)
#
#       The original example code was vague, limited and somewhat broken. It has
#       been corrected, extended, and made into a function set (performance 
#       testable) for this prototype.
#
#       The simulated data generated by the example is then visualized via
#       ggplot2, sensitive to user input regarding what predictors should
#       be used for the x-axis and/or faceting.
#
#       Our goal is to take this example and build a Shiny package on top of it.
#       We will explore how efficient it will be to simulate data from a 
#       multonomial logit model and and visualize the results on the fly via 
#       Shiny.

# sketch of script
# - LOAD SUPPORTING LIBRARIES
#
# - LOAD EXAMPLE DATA AND GET MODEL OBJECT
#   - only useful for the prototype itself (will provide model prebuilt 
#     for the final product)
#
# - FUNCTION TO GENERATE OUTCOME PREDICTIONS FOR GIVEN MODEL OBJECT
#   - extract point estimates from the model object
#   - solve for the covariance matrix
#   - get coefficient estimates via simulation
#   - generate data to feed to coefficient estimates
#   - get the outcome predictions (feed the estimates!)
#
# - FUNCTION TO VISUALIZE OUTCOME PREDICTIONS (USER SELECTED X-AXIS/FACETTING)
#
# - WRAPPER FUNCTION FOR SIMULATION AND VISUALIZATION
#   - make sure inputs are sensible (model object, original data to get ranges,
#     x-axis choice, facet choice, number of param sims, range sampling density)

###############################################################################
## LOAD SUPPORTING LIBRARIES
#   - to get simcf (a custom package on github), use the install_github 
#     function from the devtools package:
#     install_github("chrisadolph/tile-simcf", subdir = "simcf")

library(nnet)       # processes the multinomial logit
library(MASS)       # allows for multivariate sampling
library(simcf)      # creates counterfactual sets (and provides example data)
library(tidyr)      # for reformatting data for visualization
library(ggplot2)    # for visualizing the data

###############################################################################
## LOAD EXAMPLE DATA AND GET MODEL OBJECT
#   - the gator dataset comes from the simcf package (the original doc fails
#     to specify this and loaded it out of order)

# load the data
data(gator)

# adding a new continuous predictor: number of teeth
# (seed set to insure consistency during prototyping then unset to insure no interference with later
#  random number generation)
set.seed(1)
teeth <- round(runif(59, 13, 30))
set.seed = NULL

# gather into a properly formatted dataframe (useful for proper handling
# of variables later when determining variable ranges)
gator <- data.frame(food, size, sex = female, teeth)
gator$food <- factor(gator$food, 
                     levels = c(1, 2, 3), 
                     labels = c("invertebrates", "fish", "other"))
gator$sex <- factor(gator$sex, 
                    levels = c(0, 1), 
                    labels = c("male", "female"))

# quick clean-up of the no longer needed data objects
rm(female, food, set.seed, size, teeth)

# multinom: "fit a multinomial log-linear model via neural networks"
gator_logit <- multinom(food ~ size + sex + teeth, 
                        data = gator, Hess=TRUE)

# alternate test model with an interaction term
int_logit <- multinom(food ~ size + sex + teeth + size * teeth, 
                      data = gator, Hess=TRUE)

###############################################################################
## FUNCTION TO GENERATE OUTCOME PREDICTIONS FOR GIVEN MODEL OBJECT
#   - extract point estimates from the model object
#   - solve for the covariance matrix
#   - get coefficient estimates via simulation
#   - generate data to feed to coefficient estimates
#   - get the outcome predictions (feed the estimates!)

# function to extract non-reference point estimates from the model object
get_point_estimates <- function(model_object) {
    # determine the number of coefficents (intercepts, predictors, interactions) 
    # and outcomes
    number_coefficients <- length(model_object$coefnames)
    number_outcomes <- length(model_object$lab)
    
    # the multinom function returns a lot of 0s - first we find identify where the
    # non-reference weights begin (one set of weights per outcome but we 
    # skip the initial reference set)
    index_starts <- NULL
    for(i in 1:(number_outcomes - 1)) {
        # go the end of the current chunk... add two to skip the placeholder 0...
        index_starts[i] <- i * (number_coefficients + 1) + 2
    }
    
    # then we use the start indices and the the number of coefficients to
    # define the index that will align with all the non-reference weights
    wts_index <- NULL
    for(i in index_starts) {
        wts_index <- c(wts_index, i:(i + number_coefficients - 1))
    }
    
    # finally we return the point estimates (weights) for the coefficients
    return(model_object$wts[wts_index])
}

# function to get the covariance matrix from the Hessian in the model object
get_covariance_matrix <- function(model_object) {
    solve(model_object$Hess)
}

# function to get coefficient estimates via simulation
get_coefficient_estimates <- function(sample_size, 
                                      point_estimates, 
                                      covariance_matrix,
                                      number_outcomes) {
    # draw parameters, using MASS::mvrnorm
    sim_betas <- mvrnorm(sample_size, point_estimates, covariance_matrix)
    
    # data needs to be re-arranged into an array format
    # first determine array dimensions...
    number_arrays  <- number_outcomes - 1
    number_columns <- length(point_estimates)/number_arrays
    number_rows    <- sample_size
    
    # then re-arrange simulates to array format for MNL simulation
    sim_beta_array <- array(NA, dim = c(number_rows, 
                                        number_columns, 
                                        number_arrays)
                            )  
    index_starts <- seq(from = 1, to = number_columns * number_arrays, 
                        by = number_columns)
    for(i in 1:number_arrays) {
        sim_beta_array[, , i] <- sim_betas[, index_starts[i]:(index_starts[i] + number_columns - 1)]
    }
    
    # return the re-arranged coefficient estimates
    return(sim_beta_array)
}

# generate data to feed the coefficient estimates
get_new_data <- function(dataset, model_object,
                         x_axis_variable, x_range = NULL, x_range_density = 100,
                         facet_variable = NULL) {
    # check if an explicit range has been provided for the x-axis variable
    if(is.null(x_range)) {
        # if not provided, calculate the range from the dataset
        # floor and ceiling used to insure some space around the observed data
        x_range[1] <- floor(min(dataset[x_axis_variable]))
        x_range[2] <- ceiling(max(dataset[x_axis_variable]))
    }
    
    # initialize the minimum set of counterfactuals (the x-axis variable cuts)
    counterfactuals <- seq(x_range[1], x_range[2], length.out = x_range_density)
    
    # check if a facet variable has been set
    if(!is.null(facet_variable)) {
        # if a facet variable has been set, we expand the counterfactuals
        # to include all x-axis variable/facet variable combinations
        counterfactuals <- expand.grid(counterfactuals, 
                                       unique(dataset[facet_variable])
        )
        names(counterfactuals) <- c(x_axis_variable, facet_variable)
    } else {
        counterfactuals <- expand.grid(counterfactuals)
        names(counterfactuals) <- x_axis_variable
    }
    
    # finally, we check if there are additional predictors...
    variable_names <- all.vars(model_object$call[[2]])
    predictor_names <- variable_names[2:length(variable_names)]
    # by comparing the total number of predictors against the number of columns
    # in the counterfactual table...
    if(length(predictor_names) > ncol(counterfactuals)) {
        # drop out the x-axis and (if used) facet variables...
        if(!is.null(facet_variable)) {
            retained_index <- !grepl(paste(x_axis_variable, facet_variable, sep = "|"), 
                                     predictor_names)
        } else {
            retained_index <- !grepl(x_axis_variable, predictor_names)
        }
        # and gather the "fixed" predictors
        extra_predictors <- predictor_names[retained_index]
        
        # now we get the means for the fixed predictors...
        mean_set <- NULL
        for(i in 1:length(extra_predictors)) {
            mean_set[i] <- mean(as.numeric(dataset[, extra_predictors[i]]), 
                                na.rm = T)
        }
        # and attach those means to the current counterfactual set
        for(i in 1:length(extra_predictors)) {
            counterfactuals <- cbind(counterfactuals, mean_set[i])
        }
        offset_amount <- length(predictor_names) - length(extra_predictors) + 1
        names(counterfactuals)[offset_amount:ncol(counterfactuals)] <- extra_predictors
    }
    
    # we wrap up by returning the counterfactual set
    return(counterfactuals)
}


###############################################################################
## END OF SCRIPT
###############################################################################